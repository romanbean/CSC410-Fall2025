In both the array sum and matrix multiplication solutions, synchronization primitives were not required because each thread was assigned its own exclusive section of data to process, preventing any overlapping writes or shared variable updates during computation. This approach meant threads could safely operate in parallel without risking data races or inconsistent results, as all necessary aggregation or combination of results occurred only after all threads had finished and joined back with the main program. However, synchronization primitives—such as mutexes, condition variables, or barriers—would be necessary if threads ever needed to coordinate access to shared resources, update the same variables, or proceed in distinct computation phases dependent on one another, to ensure correct and consistent outcomes in the presence of concurrent execution.
