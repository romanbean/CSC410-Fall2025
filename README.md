<h1>Parallel Computing - CSC410 @ Dakota State University</h1>

- A1 - Starting with Sequential
- - An assignment where we wrote sequential programs, sum of arrays, the Queens game, bubble and merge sort algorithms, and numerical integration. Wrote a bash script to time all programs at once. Will parallelize in the next assigment.
- A2 - Part 1: A Star Wars Process and Parallel Sums
- - An assignment focuses on practicing process creation and parallel programming in C. You will complete a program that creates four child processes to adjust a shared shield power, demonstrating how processes have separate memory. Then, you will convert a sequential array summation program into a parallel version using multiple processes and pipes for communication. Finally, you will time both sequential and parallel programs and analyze the performance benefits and challenges of multiprocessing. This assignment helps build understanding of process management, inter-process communication, and parallel computation.
- A2 - Part 2: Multithreading with pthreads
- - This assignment implements parallel versions of the array sum and matrix multiplication programs from Assignment 1 using pthreads, where the array sum work is divided equally among threads and matrix multiplication threads divide work by matrix rows. Results demonstrated that parallelization overhead can outweigh benefits for simple tasks like array summation, while heavier tasks like matrix multiplication showed limited speedup but never outperformed sequential execution due to overhead and memory bottlenecks.
- A3 - Part 1: Synchronization of threads
- - This part introduces the concept and practical use of synchronization techniques to coordinate threads during execution and avoid race conditions. The assignment focuses on ensuring correct ordering and data consistency when threads share or depend on common variables. Techniques such as mutexes, condition variables, and barriers are applied to control access and enforce execution phases. Synchronization is necessary when multiple threads modify shared data or when computations require phased execution, such as waiting for all partial sums before calculating an average. These concepts build on prior multithreading exercises by adding the mechanisms needed for safe and predictable parallel program behavior.
- Midterm Part 2: Parallel N Queens and Reader-Writer Locks
- - This part of the midterm builds on earlier multithreading and synchronization concepts. The first task involves parallelizing the classic N Queens problem using pthreads. Partial board states are generated up to a certain depth (K), and each is assigned to a thread for independent backtracking, enabling exploration of solutions in parallel. The number of active threads can be easily controlled to evaluate performance across different levels of concurrency. The second task focuses on implementing a custom reader-writer lock using mutexes and condition variables. The goal is to ensure multiple readers can access shared data simultaneously, but writers gain exclusive access without causing starvation. This assignment deepens understanding of advanced thread coordination and highlights the trade-offs between safety, performance, and fairness in concurrent systems.
- A3 Part 2: Still about synchronization of threads
- - This assignment focuses on converting sequential programs to parallel versions using pthreads, emphasizing thread synchronization and performance measurement. Tasks include parallelizing numerical integration and several merge sort approaches: segment-based, recursive thread creation with limits, and thread pools to efficiently reuse threads. The assignment also examines two parallel bubble sort implementations to understand different parallelization strategies, their advantages, and limitations. Throughout the tasks, synchronization techniques such as mutexes and barriers were critical to avoid race conditions and ensure correct results when threads access shared data. Benchmarking highlighted that increasing the number of threads improves performance but often with diminishing returns due to overhead and synchronization costs. Overall, this assignment deepened understanding of pthread programming, balancing parallel efficiency with practical synchronization challenges in multithreaded applications.
- A4 From Pthreads to OpenMP :):
- - This assignment focuses on converting previously developed sequential programs into parallel versions using OpenMP, building on prior pthread implementations. The main objective is to understand and apply parallel programming concepts, thread synchronization, and performance measurement through practical coding tasks.
- A5 Now to Distributed Memory – MPI:
- - This assignment marks the transition from shared-memory programming with threads to distributed-memory parallelism using MPI (Message Passing Interface). Building on sequential and multithreaded programs developed in earlier assignments, the focus here is on converting core algorithms to run in parallel across multiple processes that communicate via messages. MPI enables coordination between processes running on different cores or even different machines, highlighting the skills required for scalable distributed computing.
- A6 Heterogeneous Computing - OpenCL:
- - This assignment continues the progression into high-performance computing by introducing heterogeneous parallelism using OpenCL. Whereas previous assignments focused on shared-memory models (Pthreads, OpenMP) and distributed-memory models (MPI), this task shifts to parallel computation on a broader range of hardware—including CPUs, GPUs, and accelerators—through OpenCL’s unified programming framework. The goal is to understand how data-parallel kernels execute on devices and how performance compares to earlier parallel programming tools.
- Final Exam Part 2:
- - This assignment focuses on parallelizing a sequential N-body simulation program (nBody.c) using multiple parallel programming tools: Pthreads, OpenMP, MPI, and OpenCL. The tasks involve converting the sequential code to run efficiently with threads, shared-memory parallelism, distributed-memory processes, and heterogeneous devices, respectively. Students benchmark each version against the sequential program to measure performance improvements and analyze the trade-offs of each parallel approach. The assignment emphasizes understanding work partitioning, synchronization, communication, and the advantages and limitations of different parallel programming models, providing hands-on experience with both shared-memory and distributed-memory parallelism as well as GPU acceleration. 
